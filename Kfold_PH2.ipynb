{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import os\n",
    "import skorch\n",
    "from skorch import NeuralNet\n",
    "import warnings\n",
    "\n",
    "# Suppress only FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sys.path.append('/zhome/45/0/155089/Deeplearning_in_computer_vision/Segmentation_project/Asignments_DeepLearningForCV/')  \n",
    "from Performance_Metrics import dice_coefficient, intersection_over_union, accuracy, sensitivity, specificity\n",
    "#import dataset PH2 \n",
    "from DataLoader_PH2 import train_loader, val_loader, test_loader, Full_set, PH2\n",
    "import time \n",
    "from time import time  # Correct import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from skorch.dataset import unpack_data\n",
    "\n",
    "\n",
    "def accuracy_scoring(net, X, y=None, sample_weight=None):\n",
    "    \"\"\"Calculate accuracy score for the net.\"\"\"\n",
    "    if sample_weight is not None:\n",
    "        raise NotImplementedError(\n",
    "            \"sample_weight for accuracy_scoring is not yet supported.\"\n",
    "        )\n",
    "\n",
    "    net.check_is_fitted()\n",
    "\n",
    "    dataset = net.get_dataset(X, y)\n",
    "    iterator = net.get_iterator(dataset, training=False)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in iterator:\n",
    "        yp = net.evaluation_step(batch, training=False)  # Model predictions\n",
    "        yi = unpack_data(batch)[1]  # Ground truth labels\n",
    "\n",
    "        # If yp is logits or probabilities, apply threshold to convert to binary (for binary segmentation)\n",
    "        # For multi-class segmentation, you may need to use argmax, depending on your task\n",
    "        if yp.shape[-1] > 1:\n",
    "            yp = yp.argmax(dim=1)  # Multi-class segmentation\n",
    "        else:\n",
    "            yp = (yp > 0.5).float()  # Binary segmentation (thresholding)\n",
    "\n",
    "        # Convert tensors to numpy arrays\n",
    "        yp = yp.detach().cpu().numpy()\n",
    "        yi = yi.detach().cpu().numpy()\n",
    "\n",
    "        # Reshape predictions and targets to 1D arrays for pixel-wise accuracy calculation\n",
    "        yp = yp.reshape(-1)\n",
    "        yi = yi.reshape(-1)\n",
    "\n",
    "        all_predictions.append(yp)\n",
    "        all_targets.append(yi)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Calculate and return accuracy\n",
    "    return accuracy_score(all_targets, all_predictions)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Check if the inputs are PyTorch tensors or NumPy arrays\n",
    "    if isinstance(y_pred, torch.Tensor) and isinstance(y_true, torch.Tensor):\n",
    "        y_pred = (y_pred > 0.5).float()  # Thresholding predictions for PyTorch\n",
    "        correct = (y_true == y_pred).float().sum()  # Correct predictions in PyTorch\n",
    "        return (correct / y_true.numel()).item()  # Return scalar accuracy for PyTorch\n",
    "\n",
    "    elif isinstance(y_pred, np.ndarray) and isinstance(y_true, np.ndarray):\n",
    "        y_pred = (y_pred > 0.5).astype(float)  # Thresholding predictions for NumPy\n",
    "        correct = np.sum(y_true == y_pred)  # Correct predictions in NumPy\n",
    "        return correct / y_true.size  # Return accuracy for NumPy\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Expected both inputs to be either PyTorch tensors or NumPy arrays\")\n",
    "\n",
    "class LossWrapper(nn.Module):\n",
    "    def __init__(self, loss_fn):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, y_real, y_pred):\n",
    "        return self.loss_fn(y_real, y_pred)\n",
    "    \n",
    "class CustomNeuralNet(NeuralNet):\n",
    "    def initialize_criterion(self):\n",
    "        if isinstance(self.criterion, nn.Module):\n",
    "            self.criterion_ = self.criterion\n",
    "        else:\n",
    "            # Initialize the wrapped loss function as a criterion\n",
    "            self.criterion_ = LossWrapper(self.criterion)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        if y is not None and isinstance(y, torch.Tensor):\n",
    "            y = y.numpy()  # Ensure y is a numpy array\n",
    "        return accuracy_scoring(self, X, y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2499\u001b[0m       \u001b[32m-0.2272\u001b[0m  0.1204\n",
      "      2        \u001b[36m0.0267\u001b[0m       -0.2249  0.1007\n",
      "      3       \u001b[36m-0.1437\u001b[0m       \u001b[32m-0.2352\u001b[0m  0.0976\n",
      "      4       \u001b[36m-0.2404\u001b[0m       \u001b[32m-0.3290\u001b[0m  0.0881\n",
      "      5       \u001b[36m-0.3103\u001b[0m       \u001b[32m-0.3781\u001b[0m  0.0859\n",
      "      6       \u001b[36m-0.3635\u001b[0m       \u001b[32m-0.4008\u001b[0m  0.0855\n",
      "      7       \u001b[36m-0.4166\u001b[0m       \u001b[32m-0.4201\u001b[0m  0.0857\n",
      "      8       \u001b[36m-0.4738\u001b[0m       \u001b[32m-0.4269\u001b[0m  0.0857\n",
      "      9       \u001b[36m-0.5564\u001b[0m       \u001b[32m-0.4658\u001b[0m  0.0856\n",
      "     10       \u001b[36m-0.7268\u001b[0m       \u001b[32m-0.5100\u001b[0m  0.0856\n",
      "     11       \u001b[36m-2.4703\u001b[0m       \u001b[32m-0.5402\u001b[0m  0.0859\n",
      "     12        0.8937       -0.1832  0.0857\n",
      "     13       -1.6911       -0.1917  0.0854\n",
      "     14        0.5567       -0.2401  0.0853\n",
      "     15        0.4008       -0.3298  0.0860\n",
      "     16        0.2306       \u001b[32m-0.5549\u001b[0m  0.0853\n",
      "     17        0.1154        1.5784  0.0855\n",
      "     18        0.0271       -0.2038  0.0851\n",
      "     19       -0.0554       -0.2874  0.0856\n",
      "     20       -0.1314       -0.2925  0.0854\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2824\u001b[0m       \u001b[32m-0.0955\u001b[0m  0.0871\n",
      "      2        \u001b[36m0.0469\u001b[0m       -0.0687  0.0859\n",
      "      3       \u001b[36m-0.1220\u001b[0m       \u001b[32m-0.1423\u001b[0m  0.0868\n",
      "      4       \u001b[36m-0.2334\u001b[0m       \u001b[32m-0.2255\u001b[0m  0.0855\n",
      "      5       \u001b[36m-0.2953\u001b[0m       \u001b[32m-0.2942\u001b[0m  0.0853\n",
      "      6       \u001b[36m-0.3418\u001b[0m       \u001b[32m-0.3385\u001b[0m  0.0854\n",
      "      7       \u001b[36m-0.3818\u001b[0m       \u001b[32m-0.3670\u001b[0m  0.0852\n",
      "      8       \u001b[36m-0.4184\u001b[0m       \u001b[32m-0.4297\u001b[0m  0.0854\n",
      "      9       \u001b[36m-0.4562\u001b[0m       \u001b[32m-0.5025\u001b[0m  0.0854\n",
      "     10       \u001b[36m-0.5149\u001b[0m       \u001b[32m-0.7470\u001b[0m  0.0854\n",
      "     11       \u001b[36m-0.6101\u001b[0m        3.3777  0.0854\n",
      "     12       \u001b[36m-0.9102\u001b[0m       \u001b[32m-0.9606\u001b[0m  0.0854\n",
      "     13       \u001b[36m-6.8803\u001b[0m       -0.1231  0.0854\n",
      "     14       -0.1343       -0.0721  0.0856\n",
      "     15       -0.3943       -0.0146  0.0853\n",
      "     16       -0.3682       -0.1671  0.0855\n",
      "     17       -0.3451       -0.1409  0.0852\n",
      "     18       -0.3423       -0.1398  0.0857\n",
      "     19       -0.3482       -0.1384  0.0852\n",
      "     20       -0.3675       -0.1395  0.0854\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2146\u001b[0m       \u001b[32m-0.1183\u001b[0m  0.0860\n",
      "      2       \u001b[36m-0.1693\u001b[0m       \u001b[32m-0.1289\u001b[0m  0.0856\n",
      "      3       \u001b[36m-0.2676\u001b[0m       \u001b[32m-0.1772\u001b[0m  0.0855\n",
      "      4       \u001b[36m-0.3325\u001b[0m       \u001b[32m-0.3185\u001b[0m  0.0853\n",
      "      5       \u001b[36m-0.3807\u001b[0m       \u001b[32m-0.3980\u001b[0m  0.0855\n",
      "      6       \u001b[36m-0.4368\u001b[0m       \u001b[32m-0.6256\u001b[0m  0.0854\n",
      "      7       \u001b[36m-0.5110\u001b[0m       \u001b[32m-0.8671\u001b[0m  0.0855\n",
      "      8       \u001b[36m-0.6626\u001b[0m       \u001b[32m-2.1649\u001b[0m  0.0852\n",
      "      9       \u001b[36m-1.9331\u001b[0m        0.3576  0.0856\n",
      "     10        2.1068       -0.1358  0.0852\n",
      "     11       -0.2894       -0.1053  0.0857\n",
      "     12       -0.3283       -0.0938  0.0854\n",
      "     13       -0.2909       -0.0818  0.0854\n",
      "     14       -0.2789       -0.0707  0.0853\n",
      "     15       -0.2785       -0.0552  0.0855\n",
      "     16       -0.2734       -0.0343  0.0853\n",
      "     17       -0.2740        0.0015  0.0852\n",
      "     18       -0.2759        0.0788  0.0851\n",
      "     19       -0.2773        0.3172  0.0852\n",
      "     20       -0.2790      \u001b[32m-23.5576\u001b[0m  0.0853\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5194\u001b[0m       \u001b[32m-0.1135\u001b[0m  0.0854\n",
      "      2        \u001b[36m0.0298\u001b[0m       \u001b[32m-0.1268\u001b[0m  0.0856\n",
      "      3       \u001b[36m-0.1107\u001b[0m       \u001b[32m-0.1809\u001b[0m  0.0856\n",
      "      4       \u001b[36m-0.2090\u001b[0m       \u001b[32m-0.2529\u001b[0m  0.0855\n",
      "      5       \u001b[36m-0.2702\u001b[0m       \u001b[32m-0.2837\u001b[0m  0.0855\n",
      "      6       \u001b[36m-0.3077\u001b[0m       \u001b[32m-0.3250\u001b[0m  0.0855\n",
      "      7       \u001b[36m-0.3381\u001b[0m       \u001b[32m-0.3465\u001b[0m  0.0852\n",
      "      8       \u001b[36m-0.3650\u001b[0m       \u001b[32m-0.3719\u001b[0m  0.0855\n",
      "      9       \u001b[36m-0.3882\u001b[0m       \u001b[32m-0.4077\u001b[0m  0.0854\n",
      "     10       \u001b[36m-0.4141\u001b[0m       \u001b[32m-0.4923\u001b[0m  0.0860\n",
      "     11       \u001b[36m-0.4392\u001b[0m       \u001b[32m-0.5365\u001b[0m  0.0855\n",
      "     12       \u001b[36m-0.4700\u001b[0m       \u001b[32m-0.5945\u001b[0m  0.0856\n",
      "     13       \u001b[36m-0.5085\u001b[0m       -0.5862  0.0854\n",
      "     14       \u001b[36m-0.5701\u001b[0m        5.8942  0.0856\n",
      "     15       \u001b[36m-0.6898\u001b[0m       -0.2699  0.0854\n",
      "     16       \u001b[36m-1.3769\u001b[0m        0.0162  0.0855\n",
      "     17        1.0341       -0.0746  0.0852\n",
      "     18       -0.3227       -0.0727  0.0855\n",
      "     19       -0.2739       -0.0710  0.0853\n",
      "     20       -0.2574       -0.0602  0.0857\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.4659\u001b[0m       \u001b[32m-4.4387\u001b[0m  0.0857\n",
      "      2        \u001b[36m0.3498\u001b[0m       -0.0180  0.0855\n",
      "      3        \u001b[36m0.1368\u001b[0m       -0.1127  0.0857\n",
      "      4       \u001b[36m-0.0661\u001b[0m       -0.1819  0.0856\n",
      "      5       \u001b[36m-0.2179\u001b[0m       -0.2562  0.0855\n",
      "      6       \u001b[36m-0.2921\u001b[0m       -0.3090  0.0853\n",
      "      7       \u001b[36m-0.3398\u001b[0m       -0.2159  0.0854\n",
      "      8       \u001b[36m-0.3778\u001b[0m       -0.3382  0.0853\n",
      "      9       \u001b[36m-0.4120\u001b[0m       -0.4289  0.0854\n",
      "     10       \u001b[36m-0.4497\u001b[0m       -0.4731  0.0852\n",
      "     11       \u001b[36m-0.5002\u001b[0m       -1.0506  0.0856\n",
      "     12       \u001b[36m-0.5857\u001b[0m       -0.1084  0.0853\n",
      "     13       \u001b[36m-0.8243\u001b[0m       -0.4632  0.0857\n",
      "     14       \u001b[36m-1.0285\u001b[0m       -0.0231  0.0857\n",
      "     15       -0.8037       -1.7137  0.0857\n",
      "     16        0.1790       -0.0979  0.0856\n",
      "     17        0.0300       -0.1260  0.0856\n",
      "     18        0.0090       -0.0211  0.0853\n",
      "     19       -0.0016       -0.0577  0.0851\n",
      "     20       -0.0108       -0.0589  0.0854\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m335.8152\u001b[0m      \u001b[32m328.7288\u001b[0m  0.0859\n",
      "      2      \u001b[36m335.3019\u001b[0m      334.7690  0.0858\n",
      "      3      \u001b[36m334.1106\u001b[0m      \u001b[32m327.6675\u001b[0m  0.0857\n",
      "      4      \u001b[36m332.0596\u001b[0m      \u001b[32m322.4203\u001b[0m  0.0856\n",
      "      5      \u001b[36m329.1932\u001b[0m      \u001b[32m320.3899\u001b[0m  0.0854\n",
      "      6      \u001b[36m323.8700\u001b[0m      \u001b[32m298.8295\u001b[0m  0.0857\n",
      "      7      \u001b[36m317.2680\u001b[0m      \u001b[32m266.9662\u001b[0m  0.0855\n",
      "      8      \u001b[36m308.1034\u001b[0m      \u001b[32m256.3897\u001b[0m  0.0856\n",
      "      9      \u001b[36m296.1522\u001b[0m      \u001b[32m247.1324\u001b[0m  0.0855\n",
      "     10      \u001b[36m279.5965\u001b[0m      \u001b[32m214.7907\u001b[0m  0.0857\n",
      "     11      \u001b[36m258.7345\u001b[0m      \u001b[32m205.6641\u001b[0m  0.0856\n",
      "     12      \u001b[36m234.9110\u001b[0m      \u001b[32m118.3165\u001b[0m  0.0858\n",
      "     13      \u001b[36m201.8359\u001b[0m      141.1841  0.0860\n",
      "     14      \u001b[36m164.3336\u001b[0m       \u001b[32m10.0317\u001b[0m  0.0856\n",
      "     15      \u001b[36m113.1227\u001b[0m      \u001b[32m-31.1951\u001b[0m  0.0854\n",
      "     16       \u001b[36m55.0666\u001b[0m      \u001b[32m-32.3943\u001b[0m  0.0855\n",
      "     17      \u001b[36m-13.9991\u001b[0m     \u001b[32m-169.7314\u001b[0m  0.0855\n",
      "     18     \u001b[36m-101.3557\u001b[0m     \u001b[32m-297.8310\u001b[0m  0.0855\n",
      "     19     \u001b[36m-190.0202\u001b[0m     \u001b[32m-646.3882\u001b[0m  0.0856\n",
      "     20     \u001b[36m-312.9478\u001b[0m     \u001b[32m-774.9902\u001b[0m  0.0857\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m346.2290\u001b[0m     \u001b[32m-642.3685\u001b[0m  0.0856\n",
      "      2      \u001b[36m344.4819\u001b[0m      195.5161  0.0858\n",
      "      3      \u001b[36m341.9652\u001b[0m      239.8166  0.0857\n",
      "      4      \u001b[36m338.4840\u001b[0m      235.6156  0.0857\n",
      "      5      \u001b[36m333.5700\u001b[0m      238.1316  0.0861\n",
      "      6      \u001b[36m327.0909\u001b[0m      225.9287  0.0856\n",
      "      7      \u001b[36m318.2211\u001b[0m      235.5815  0.0855\n",
      "      8      \u001b[36m307.3577\u001b[0m      206.4442  0.0856\n",
      "      9      \u001b[36m293.1794\u001b[0m      143.8917  0.0856\n",
      "     10      \u001b[36m273.9883\u001b[0m      223.2286  0.0857\n",
      "     11      \u001b[36m249.4323\u001b[0m       84.9318  0.0859\n",
      "     12      \u001b[36m216.3949\u001b[0m       37.8172  0.0858\n",
      "     13      \u001b[36m181.7200\u001b[0m      -67.5168  0.0858\n",
      "     14      \u001b[36m134.0759\u001b[0m       69.2103  0.0856\n",
      "     15       \u001b[36m83.1725\u001b[0m      -35.3900  0.0855\n",
      "     16       \u001b[36m12.6192\u001b[0m      -48.6295  0.0854\n",
      "     17      \u001b[36m-60.1004\u001b[0m      -58.0492  0.0854\n",
      "     18     \u001b[36m-154.8845\u001b[0m      -21.4359  0.0857\n",
      "     19     \u001b[36m-255.4259\u001b[0m     -382.5948  0.0857\n",
      "     20     \u001b[36m-362.3679\u001b[0m     -631.5973  0.0855\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m345.4475\u001b[0m      \u001b[32m155.9050\u001b[0m  0.0862\n",
      "      2      \u001b[36m345.1076\u001b[0m      228.7399  0.0860\n",
      "      3      \u001b[36m344.1775\u001b[0m      242.8155  0.0857\n",
      "      4      \u001b[36m342.5760\u001b[0m      231.0750  0.0858\n",
      "      5      \u001b[36m340.0670\u001b[0m      233.6940  0.0855\n",
      "      6      \u001b[36m336.3923\u001b[0m      226.2746  0.0856\n",
      "      7      \u001b[36m331.3941\u001b[0m      202.6922  0.0855\n",
      "      8      \u001b[36m323.8665\u001b[0m      211.4358  0.0857\n",
      "      9      \u001b[36m312.1917\u001b[0m      180.3974  0.0859\n",
      "     10      \u001b[36m297.9272\u001b[0m       \u001b[32m31.6026\u001b[0m  0.0858\n",
      "     11      \u001b[36m279.1970\u001b[0m       \u001b[32m13.5419\u001b[0m  0.0857\n",
      "     12      \u001b[36m255.3958\u001b[0m      \u001b[32m-83.8760\u001b[0m  0.0858\n",
      "     13      \u001b[36m225.1995\u001b[0m      -28.1572  0.0859\n",
      "     14      \u001b[36m187.7779\u001b[0m      -70.6116  0.0856\n",
      "     15      \u001b[36m142.4381\u001b[0m       15.6186  0.0858\n",
      "     16       \u001b[36m86.7876\u001b[0m      \u001b[32m-84.2715\u001b[0m  0.0855\n",
      "     17       \u001b[36m18.5645\u001b[0m      \u001b[32m-86.2943\u001b[0m  0.0859\n",
      "     18      \u001b[36m-58.4640\u001b[0m     \u001b[32m-109.9022\u001b[0m  0.0859\n",
      "     19     \u001b[36m-154.2863\u001b[0m     \u001b[32m-120.6974\u001b[0m  0.0857\n",
      "     20     \u001b[36m-247.8707\u001b[0m     \u001b[32m-594.8945\u001b[0m  0.0858\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m371.5335\u001b[0m      \u001b[32m146.3408\u001b[0m  0.0859\n",
      "      2      \u001b[36m370.5361\u001b[0m      227.3787  0.0858\n",
      "      3      \u001b[36m368.9430\u001b[0m      241.3319  0.0858\n",
      "      4      \u001b[36m366.4555\u001b[0m      216.3724  0.0857\n",
      "      5      \u001b[36m362.8975\u001b[0m      234.1018  0.0857\n",
      "      6      \u001b[36m357.9157\u001b[0m      205.9157  0.0859\n",
      "      7      \u001b[36m350.7425\u001b[0m      211.3825  0.0858\n",
      "      8      \u001b[36m339.7185\u001b[0m      186.3805  0.0856\n",
      "      9      \u001b[36m325.4031\u001b[0m      \u001b[32m144.7065\u001b[0m  0.0857\n",
      "     10      \u001b[36m307.1553\u001b[0m       \u001b[32m81.6291\u001b[0m  0.0858\n",
      "     11      \u001b[36m283.8648\u001b[0m      115.3492  0.0856\n",
      "     12      \u001b[36m255.2277\u001b[0m      186.3087  0.0858\n",
      "     13      \u001b[36m220.1259\u001b[0m       \u001b[32m58.0410\u001b[0m  0.0855\n",
      "     14      \u001b[36m173.8055\u001b[0m       67.0597  0.0857\n",
      "     15      \u001b[36m124.5626\u001b[0m      111.6925  0.0856\n",
      "     16       \u001b[36m59.5001\u001b[0m      \u001b[32m-64.8972\u001b[0m  0.0857\n",
      "     17      \u001b[36m-17.6499\u001b[0m      -31.1462  0.0856\n",
      "     18     \u001b[36m-108.0781\u001b[0m     \u001b[32m-131.3505\u001b[0m  0.0859\n",
      "     19     \u001b[36m-214.2695\u001b[0m      -35.8446  0.0857\n",
      "     20     \u001b[36m-336.3537\u001b[0m     \u001b[32m-195.2418\u001b[0m  0.0862\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m333.9897\u001b[0m     \u001b[32m-894.6182\u001b[0m  0.0860\n",
      "      2      \u001b[36m332.9221\u001b[0m      239.6163  0.0860\n",
      "      3      \u001b[36m331.1650\u001b[0m      239.8437  0.0862\n",
      "      4      \u001b[36m328.4983\u001b[0m      245.6037  0.0861\n",
      "      5      \u001b[36m324.5823\u001b[0m      231.2190  0.0862\n",
      "      6      \u001b[36m318.8914\u001b[0m      234.6885  0.0874\n",
      "      7      \u001b[36m310.8009\u001b[0m      234.0133  0.0870\n",
      "      8      \u001b[36m298.9764\u001b[0m      193.9015  0.0861\n",
      "      9      \u001b[36m283.8759\u001b[0m      212.3924  0.0863\n",
      "     10      \u001b[36m264.8907\u001b[0m       71.7986  0.0863\n",
      "     11      \u001b[36m240.4006\u001b[0m      124.4488  0.0863\n",
      "     12      \u001b[36m210.5667\u001b[0m      -67.3024  0.0862\n",
      "     13      \u001b[36m174.6280\u001b[0m      -58.2763  0.0861\n",
      "     14      \u001b[36m127.9378\u001b[0m      155.4209  0.0861\n",
      "     15       \u001b[36m72.5429\u001b[0m      -91.9473  0.0861\n",
      "     16        \u001b[36m8.5969\u001b[0m       15.4381  0.0861\n",
      "     17      \u001b[36m-64.9604\u001b[0m     -201.6468  0.0862\n",
      "     18     \u001b[36m-154.6919\u001b[0m     -548.9666  0.0862\n",
      "     19     \u001b[36m-255.7821\u001b[0m     -380.5521  0.0863\n",
      "     20     \u001b[36m-379.2048\u001b[0m     -341.0629  0.0861\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2338\u001b[0m       \u001b[32m-0.1170\u001b[0m  0.1030\n",
      "      2       \u001b[36m-0.0030\u001b[0m       -0.1157  0.1008\n",
      "      3       \u001b[36m-0.1673\u001b[0m       \u001b[32m-0.1200\u001b[0m  0.1027\n",
      "      4       \u001b[36m-0.2656\u001b[0m       \u001b[32m-0.1963\u001b[0m  0.1011\n",
      "      5       \u001b[36m-0.3308\u001b[0m       \u001b[32m-0.3293\u001b[0m  0.1014\n",
      "      6       \u001b[36m-0.3793\u001b[0m       \u001b[32m-0.3791\u001b[0m  0.1008\n",
      "      7       \u001b[36m-0.4201\u001b[0m       \u001b[32m-0.4027\u001b[0m  0.1015\n",
      "      8       \u001b[36m-0.4636\u001b[0m       \u001b[32m-0.4143\u001b[0m  0.1004\n",
      "      9       \u001b[36m-0.5206\u001b[0m       \u001b[32m-0.4453\u001b[0m  0.1009\n",
      "     10       \u001b[36m-0.6172\u001b[0m       \u001b[32m-0.5780\u001b[0m  0.1003\n",
      "     11       \u001b[36m-0.8949\u001b[0m        0.1873  0.1004\n",
      "     12       \u001b[36m-2.9561\u001b[0m       \u001b[32m-0.8813\u001b[0m  0.1006\n",
      "     13        0.8465        0.0649  0.1004\n",
      "     14       -0.2302       -0.1247  0.1008\n",
      "     15       -0.4643       -0.1215  0.1011\n",
      "     16       -0.4059       -0.1292  0.1007\n",
      "     17       -0.4019       -0.1516  0.1022\n",
      "     18       -0.4285       -0.1567  0.1010\n",
      "     19       -0.4653       -0.1514  0.1023\n",
      "     20       -0.4989       -0.1709  0.1012\n"
     ]
    }
   ],
   "source": [
    "from Encoder_Decoder_PH2 import EncDec\n",
    "from functools import partial\n",
    "from UNet2 import bce_loss, dice_loss, focal_loss, bce_total_variation, focal_loss_chatten, stable_bce_loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Wrapping your custom loss functions using functools.partial\n",
    "param_grid = {\n",
    "    'criterion': [\n",
    "    dice_loss, \n",
    "    # focal_loss, \n",
    "    bce_total_variation, \n",
    "    # focal_loss_chatten, \n",
    "    # stable_bce_loss\n",
    "],\n",
    "    # 'max_epochs':[15,\n",
    "    #                 20,\n",
    "    #                 25,\n",
    "    #                 30],\n",
    "    # 'batch_size': [10,\n",
    "    #                20,\n",
    "    #                30,\n",
    "    #                40]\n",
    "              }\n",
    "\n",
    "# Initialize model\n",
    "model = CustomNeuralNet(EncDec, \n",
    "                  optimizer=optim.Adam, \n",
    "                  verbose=1,\n",
    "                  max_epochs=20,\n",
    "                  batch_size=40,\n",
    "                #   score = dice_coefficient,\n",
    "                  criterion=bce_loss,  # A placeholder, will be overwritten in GridSearchCV\n",
    "                  device=device,\n",
    "                  iterator_train__shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=5,)\n",
    "\n",
    "# Assuming you have a dataset called PH2\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
    "Full_set = PH2(transform=train_transform)\n",
    "\n",
    "X = torch.stack([X for X,_ in Full_set])\n",
    "y = torch.stack([y for X,y in Full_set])\n",
    "\n",
    "\n",
    "# dataset = CustomDataset(Full_set)\n",
    "# # Fit grid search\n",
    "grid_result = grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': <function UNet2.dice_loss(y_real, y_pred)>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6776934814453125)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.677693 using {'criterion': <function dice_loss at 0x7f7cc3465a80>}\n",
      "0.677693 (0.096186) with: {'criterion': <function dice_loss at 0x7f7cc3465a80>}\n",
      "0.677693 (0.096186) with: {'criterion': <function bce_total_variation at 0x7f7cc3464f40>}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
